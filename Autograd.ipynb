{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd\n",
        "- Autograd is a core component of PyTorch that provides automatic differentiation for tensor operations. It enables gradient computation which is essential for training machine learning models using optimization algorithm like gradient descent.\n",
        "- Gradient:  It indicates the direction and magnitude of the steepest increase of that function.\n",
        "- Let's take a simple function:\n",
        "`f(x, y) = x²y + y³`\n",
        "\n"
      ],
      "metadata": {
        "id": "vpf5budDDyYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "KeelRAgSEQLx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "# requires_grad=True tells PyTorch:\n",
        "# \"Track all operations on this tensor for gradient calculation!\"\n",
        "y = torch.tensor(3.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "ulgWetmFEayh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = x**2 * y + y**3\n",
        "# z = (2)² × 3 + (3)³\n",
        "#   = 4 × 3 + 27\n",
        "#   = 12 + 27 = 39"
      ],
      "metadata": {
        "id": "2OQqJQ--EfXi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()  # Computes ∂z/∂x and ∂z/∂y"
      ],
      "metadata": {
        "id": "PUN8uITrEiTA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial Derivatives:\n",
        "\n",
        "- `∂z/∂x` = 2xy = 2×2×3 = 12\n",
        "- `∂z/∂y` = x² + 3y² = 4 + 27 = 31"
      ],
      "metadata": {
        "id": "OG9EsBFsEkwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0UA8kN-EwHX",
        "outputId": "54107c7a-f614-44b0-db5b-22d5157bd7ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12.)\n",
            "tensor(31.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it calculates derivative for us.\n",
        "# Why this matters in ML:\n",
        "\n",
        "In neural networks, we have:\n",
        "`Loss = f(weights, biases)`\n",
        "\n",
        "Autograd automatically computes:\n",
        "\n",
        "`∂Loss/∂weights`\n",
        "\n",
        "`∂Loss/∂biases`\n",
        "\n",
        "So we can update: `weights = weights - η × ∂Loss/∂weights`\n",
        "\n",
        "No manual calculus needed! PyTorch tracks all operations and applies chain rule automatically."
      ],
      "metadata": {
        "id": "uWijLTSsE12w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Multiple operations chain\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(2.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "M0mS-lXiFLB6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass: y = w*x + b (with x=3)\n",
        "x = 3.0\n",
        "y_pred = w * x + b        # y_pred = 1*3 + 2 = 5\n",
        "y_true = 10.0"
      ],
      "metadata": {
        "id": "iOW-WIO5FazC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss = (y_pred - y_true)²\n",
        "loss = (y_pred - y_true) ** 2  # (5-10)² = 25"
      ],
      "metadata": {
        "id": "W0OME2uVFcrL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd magic!\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "703bPrWZFfQy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"∂loss/∂w:\", w.grad)  # 2*(y_pred-y_true)*x = 2*(-5)*3 = -30\n",
        "print(\"∂loss/∂b:\", b.grad)  # 2*(y_pred-y_true)*1 = 2*(-5) = -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmVTGFhMFhrs",
        "outputId": "51a7ef80-b4e6-4f06-92d7-c4ff4214cc90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "∂loss/∂w: tensor(-30.)\n",
            "∂loss/∂b: tensor(-10.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " `.backward()`:\n",
        "- Computes derivatives using the chain rule\n",
        "- Starts from loss and works backward through all operations\n",
        "- Calculates: `∂loss/∂x` for every tensor with `requires_grad=True`\n",
        "\n",
        "`.grad`:\n",
        "- Stores the result of the derivative\n",
        "- Contains: ∂loss/∂x (how much loss changes when x changes)"
      ],
      "metadata": {
        "id": "Wu7K9ybcGVkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2                    # y = x²\n",
        "y.backward()                # ← COMPUTES derivative\n",
        "print(x.grad)               # ← STORES derivative result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUtz4QIeGjX3",
        "outputId": "8acac510-65ad-4913-de8d-ff9255d32eac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd in Vector Input Tensors\n"
      ],
      "metadata": {
        "id": "K3TBIdjoJHN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Vector input instead of scalar"
      ],
      "metadata": {
        "id": "kufEdR5UJVfR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = (x ** 2).mean()"
      ],
      "metadata": {
        "id": "6TScywsAJYeh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "qlmfEFSbJcmK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKP5rUuGJrIJ",
        "outputId": "75ddf45d-9034-4f8b-a17b-f9b00327302c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`y = mean(x²) = (x₁² + x₂² + x₃²)/3`\n",
        "\n",
        "Partial derivatives:\n",
        "\n",
        "`∂y/∂x₁ = 2x₁/3 = 2×1/3 = 0.6667`\n",
        "\n",
        "`∂y/∂x₂ = 2x₂/3 = 2×2/3 = 1.3333`\n",
        "\n",
        "`∂y/∂x₃ = 2x₃/3 = 2×3/3 = 2.0000`"
      ],
      "metadata": {
        "id": "v_8PRIhiJ5vL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradients accumulate by default in PyTorch. If you don't clear them, each `.backward()` call adds to previous gradients."
      ],
      "metadata": {
        "id": "BludooOxKG0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "cj_3w6E1KJsX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First backward pass\n",
        "y = x ** 2\n",
        "y.backward()\n",
        "print(\"Grad after 1st:\", x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqvx0b6BKlND",
        "outputId": "bd450613-7b12-4673-c242-999e12db9dd1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad after 1st: tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear gradients\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RU6n90ZKnV0",
        "outputId": "e57a53fc-af5e-4561-d241-6dc84ed67643"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second backward pass\n",
        "y = x ** 3\n",
        "y.backward()\n",
        "print(\"Grad after 2nd:\", x.grad) # tensor(12.0) - NOT 16.0!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9alAan5KoPB",
        "outputId": "66fea59f-53b0-4f46-9351-a3f3f9c021ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad after 2nd: tensor(12.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option to disable Gradient Tracking"
      ],
      "metadata": {
        "id": "fGtE6V9UK8NU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default: gradient tracking ON\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Gradient tracking OFF\n",
        "x = torch.tensor(2.0, requires_grad=False)  # Default behavior\n",
        "x = torch.tensor(2.0)  # Same as above"
      ],
      "metadata": {
        "id": "14is41VlLCMC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2  # y has gradient tracking\n",
        "\n",
        "x_detached = x.detach()  # Remove gradient tracking\n",
        "z = x_detached ** 3     # z has NO gradient tracking"
      ],
      "metadata": {
        "id": "0aDsYomALF2g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "with torch.no_grad():  # All operations inside have NO gradient tracking\n",
        "    y = x ** 2\n",
        "    z = y + 1\n",
        "# y and z don't require grad\n",
        "\n",
        "# Outside: gradient tracking resumes\n",
        "w = x ** 3  # w requires grad"
      ],
      "metadata": {
        "id": "uxWpuaO9LIvx"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}