{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Gradient Descent\n",
        "Batch Gradient Descent is an optimization algorithm where you use the entire training dataset to compute the gradient of the loss function in each iteration.\n",
        "\n",
        "It is usually avoided because it is memory inefficient and it doesn't have better convergence. We were previously using Batch Gradient Descent, We need to fix it.\n",
        "\n",
        "So instead of loading dataset in one we can divide them in batches. This is called **Mini Batch Gradient Descent**.\n",
        "\n",
        "Our current approach uses Batch GD (entire dataset at once), which is inefficient. With DataLoader, we can easily implement Mini-Batch GD by:\n",
        "\n",
        "- Creating batches of optimal size\n",
        "- Processing one batch per iteration\n",
        "- Updating parameters after each batch\n",
        "# Dataset and DataLoader\n",
        "Dataset and DataLoader are core abstraction in pytorch that decouple how you define your data from how you efficiently iterate over it in training loops.\n",
        "\n",
        "Dataset Class job is to load the data and DataLoader Class job is to create batches from loaded Data.\n",
        "\n",
        "Eg: There is CSV dataset with 10 rows and batch size is 2 meaning total batches will be 10/2 = 5 batches.\n",
        "\n",
        "Dataset class is essentially a blueprint that defines how to access and preprocess your raw data. It defines:\n",
        "\n",
        "- `__init__(self, ...)`: Initializes the dataset - sets up data paths, parameters, and preprocessing rules.\n",
        "\n",
        "- `__getitem__(self, index)`: Retrieves and returns the single data sample at the given index, including any transformations.\n",
        "\n",
        "- `__len__(self)`: Returns the total number of samples in the dataset.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data):  # Setup data\n",
        "        self.data = data\n",
        "    \n",
        "    def __getitem__(self, index):  # Get one sample\n",
        "        return self.data[index]\n",
        "    \n",
        "    def __len__(self):  # Total samples\n",
        "        return len(self.data)\n",
        "\n",
        "# Example data: 10 numbers\n",
        "data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "dataset = SimpleDataset(data)\n",
        "\n",
        "# Test the dataset\n",
        "print(f\"Total samples: {len(dataset)}\")  # Output: Total samples: 10\n",
        "print(f\"Sample at index 0: {dataset[0]}\")  # Output: Sample at index 0: 10\n",
        "print(f\"Sample at index 5: {dataset[5]}\")  # Output: Sample at index 5: 60\n",
        "```\n",
        "\n",
        "The DataLoader is a powerful iterator that works with your Dataset to automatically:\n",
        "\n",
        "- Create batches from individual samples\n",
        "- Shuffle data randomly each epoch\n",
        "- Load data in parallel using multiple workers\n",
        "- Manage memory efficiently\n",
        "## Features:\n",
        "- Batching: Groups individual samples into mini-batches for efficient processing\n",
        "- Shuffling: Randomizes data order to prevent learning sequence patterns\n",
        "- Parallel Loading: Uses multiple workers to load next batch while current one processes\n",
        "- Automatic Batching: Handles different batch sizes and dataset lengths\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Same Simple Dataset from before\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Create dataset with 10 samples\n",
        "data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "dataset = SimpleDataset(data)\n",
        "\n",
        "# Create DataLoader with batch_size=2\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Iterate through batches\n",
        "print(\"Batches with batch_size=2:\")\n",
        "for batch_idx, batch in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx}: {batch}\")\n",
        "\n",
        "# Output will look like (order changes due to shuffle=True):\n",
        "# Batch 0: tensor([20, 90])  \n",
        "# Batch 1: tensor([40, 30])\n",
        "# Batch 2: tensor([10, 70])\n",
        "# Batch 3: tensor([60, 80])\n",
        "# Batch 4: tensor([100, 50])\n",
        "```"
      ],
      "metadata": {
        "id": "Nt9ohbP1-hZa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iiIDuNwc-bhj"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a synthetic classification dataset using sklearn\n",
        "X, y = make_classification(\n",
        "    n_samples=10,       # Number of samples\n",
        "    n_features=2,       # Number of features\n",
        "    n_informative=2,    # Number of informative features\n",
        "    n_redundant=0,      # Number of redundant features\n",
        "    n_classes=2,        # Number of classes\n",
        "    random_state=42     # For reproducibility\n",
        ")"
      ],
      "metadata": {
        "id": "8VnVTt1Z-3wk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uYUOa4H-8r2",
        "outputId": "2f263774-fbc1-400b-dcbf-81e4bebcc543"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.06833894, -0.97007347],\n",
              "       [-1.14021544, -0.83879234],\n",
              "       [-2.8953973 ,  1.97686236],\n",
              "       [-0.72063436, -0.96059253],\n",
              "       [-1.96287438, -0.99225135],\n",
              "       [-0.9382051 , -0.54304815],\n",
              "       [ 1.72725924, -1.18582677],\n",
              "       [ 1.77736657,  1.51157598],\n",
              "       [ 1.89969252,  0.83444483],\n",
              "       [-0.58723065, -1.97171753]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "ePVQ_lRL-97l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "A6z4niWO_AOI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels):\n",
        "\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return self.features.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    return self.features[index], self.labels[index] # Returns Row of Index position that is given as input"
      ],
      "metadata": {
        "id": "IR8dSUit_F2k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(X, y)"
      ],
      "metadata": {
        "id": "QL-l-Wkw_cJ1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "def8S4pZ_m0n",
        "outputId": "17e7f209-3b7f-4f24-e9a6-0c2daff04b0c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmwF9g6k_n-i",
        "outputId": "1f63753b-ef95-4261-9c2e-a67612c7917d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-2.8954,  1.9769]), tensor(0))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)"
      ],
      "metadata": {
        "id": "6mDv-Nmv_r8L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_features, batch_labels in dataloader:\n",
        "\n",
        "  print(batch_features)\n",
        "  print(batch_labels)\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t39b7L83_tih",
        "outputId": "adbae50b-5979-4510-b41f-d3bf753c906c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.0683, -0.9701],\n",
            "        [-1.1402, -0.8388]])\n",
            "tensor([1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-2.8954,  1.9769],\n",
            "        [-0.7206, -0.9606]])\n",
            "tensor([0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.9629, -0.9923],\n",
            "        [-0.9382, -0.5430]])\n",
            "tensor([0, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.7273, -1.1858],\n",
            "        [ 1.7774,  1.5116]])\n",
            "tensor([1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.8997,  0.8344],\n",
            "        [-0.5872, -1.9717]])\n",
            "tensor([1, 0])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workers are subprocesses that load data in parallel while the main process trains the model. This prevents the GPU from waiting for data.\n",
        "\n",
        "`num_workers=0`: Main process loads data (GPU waits)\n",
        "\n",
        "`num_workers=2`: 2 subprocesses load data concurrently (GPU rarely waits)"
      ],
      "metadata": {
        "id": "JaEgNaOoAm2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SlowDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        time.sleep(0.1)  # Simulate slow data loading\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Create dataset\n",
        "data = list(range(10))\n",
        "dataset = SlowDataset(data)\n",
        "\n",
        "# WITHOUT workers (slow)\n",
        "print(\"Without workers (num_workers=0):\")\n",
        "dataloader_slow = DataLoader(dataset, batch_size=2, num_workers=0)\n",
        "start = time.time()\n",
        "for batch in dataloader_slow:\n",
        "    print(f\"Batch: {batch}\")\n",
        "print(f\"Time: {time.time() - start:.2f}s\\n\")\n",
        "\n",
        "# WITH workers (fast)\n",
        "print(\"With workers (num_workers=2):\")\n",
        "dataloader_fast = DataLoader(dataset, batch_size=2, num_workers=2)\n",
        "start = time.time()\n",
        "for batch in dataloader_fast:\n",
        "    print(f\"Batch: {batch}\")\n",
        "print(f\"Time: {time.time() - start:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNvamzYjAvKo",
        "outputId": "11a8c5f5-ce18-49a0-8006-3eb745f4a8da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without workers (num_workers=0):\n",
            "Batch: tensor([0, 1])\n",
            "Batch: tensor([2, 3])\n",
            "Batch: tensor([4, 5])\n",
            "Batch: tensor([6, 7])\n",
            "Batch: tensor([8, 9])\n",
            "Time: 1.01s\n",
            "\n",
            "With workers (num_workers=2):\n",
            "Batch: tensor([0, 1])\n",
            "Batch: tensor([2, 3])\n",
            "Batch: tensor([4, 5])\n",
            "Batch: tensor([6, 7])\n",
            "Batch: tensor([8, 9])\n",
            "Time: 0.85s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling (Shuffling)\n",
        "Sampling controls the order in which data samples are selected from the dataset.\n",
        "\n",
        "## Types of Samplers:\n",
        "- SequentialSampler: Samples in sequential order (0, 1, 2, 3...)\n",
        "\n",
        "- RandomSampler: Samples in random order (shuffling)\n",
        "\n",
        "- WeightedRandomSampler: Samples with probability weights"
      ],
      "metadata": {
        "id": "Xmcbh6-uBXMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "\n",
        "data = [10, 20, 30, 40, 50]\n",
        "dataset = torch.utils.data.TensorDataset(torch.tensor(data))\n",
        "\n",
        "# Sequential Sampling (for validation/test)\n",
        "sequential_loader = DataLoader(dataset, batch_size=2, sampler=SequentialSampler(dataset))\n",
        "print(\"Sequential Sampling:\")\n",
        "for batch in sequential_loader:\n",
        "    print(batch)  # Always: [10, 20], [30, 40], [50]\n",
        "\n",
        "# Random Sampling (for training)\n",
        "random_loader = DataLoader(dataset, batch_size=2, sampler=RandomSampler(dataset))\n",
        "print(\"\\nRandom Sampling:\")\n",
        "for batch in random_loader:\n",
        "    print(batch)  # Random order: [30, 10], [40, 50], [20] etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPk3WM8MBimY",
        "outputId": "03d04172-ce83-458e-aa1f-4e66aab5b0fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential Sampling:\n",
            "[tensor([10, 20])]\n",
            "[tensor([30, 40])]\n",
            "[tensor([50])]\n",
            "\n",
            "Random Sampling:\n",
            "[tensor([20, 40])]\n",
            "[tensor([50, 10])]\n",
            "[tensor([30])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collate Function\n",
        "Collate Function defines how individual samples are combined into a batch.\n",
        "\n",
        "Default Collate Behavior:\n",
        "- Stacks tensors with same shape\n",
        "- Creates lists for different-shaped items\n",
        "- Converts numbers to tensors"
      ],
      "metadata": {
        "id": "MNJ4YXSHBqJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "class CustomDataset:\n",
        "    def __len__(self): return 3\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor([idx, idx*2])  # Sample: [0,0], [1,2], [2,4]\n",
        "\n",
        "dataset = CustomDataset()\n",
        "\n",
        "# Default collate stacks tensors\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "for batch in dataloader:\n",
        "    print(f\"Batch shape: {batch.shape}, Batch: {batch}\")\n",
        "    # Output: Batch shape: torch.Size([2, 2]), Batch: tensor([[0, 0], [1, 2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzGUqtJXB1Of",
        "outputId": "b9a53a38-04a9-4170-a7b1-1bd191d937a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: torch.Size([2, 2]), Batch: tensor([[0, 0],\n",
            "        [1, 2]])\n",
            "Batch shape: torch.Size([1, 2]), Batch: tensor([[2, 4]])\n"
          ]
        }
      ]
    }
  ]
}