# TorchTales
Learning &amp; Building in Pytorch

- **Tensors Basics**: [View Notebook](https://github.com/At0mXploit/TorchTales/blob/main/Tensors.ipynb) - Learning about Tensors in PyTorch
- **Autograd**: [View Notebook](https://github.com/At0mXploit/TorchTales/blob/main/Autograd.ipynb) - Learning about Autograd and how they help in ML during Forward Pass, Backward Pass.
- **Training Pipeline:** [View Notebook](https://github.com/At0mXploit/TorchTales/blob/main/TrainingPipelineNeuralNetwork.ipynb) - Building a Single Neuron Neural Network with Training Pipeline for Learning.
- **NN Modules**: [View Notebook](https://github.com/At0mXploit/TorchTales/tree/main/NN%20Module) - Building NN with help of NN Modules.
- **Dataset & DataLoader**: [View Notebook](https://github.com/At0mXploit/TorchTales/tree/main/Dataset%20%26%20DataLoader) - Theory on Dataset & DataLoader and Mini Batch Gradient Descent also Improving our code.
- **ANN (Artificial Neural Network) / MLP (Multi Layer Perceptron)**: [View Notebook](https://github.com/At0mXploit/TorchTales/tree/main/ANN) - Buidling ANN without GPU with and without GPU of [MNIST Fashion Dataset](https://www.kaggle.com/datasets/zalando-research/fashionmnist). 
- **ANN Optimization**: [View Notebook](https://github.com/At0mXploit/TorchTales/tree/main/ANN%20Optimization) - Optimizing ANN to reduce Overfitting using Dropouts, Batch Normalization and L2 Regularization.
- **Hyperparameter Tuning & Bayesian Optimization (Optuna terms)**: [View PDF](https://github.com/At0mXploit/TorchTales/blob/main/Hyperparameter%20Tuning%20and%20Bayesian%20Optimization.pdf) - Grid Search CV and Random Search CV are techniques used for hyperparameter tuning but Optuna uses Bayesian Optimization by default.
- **Optuna Basics**: [View Notebook](https://github.com/At0mXploit/TorchTales/blob/main/Optuna.ipynb) - Applying hyperparameter tuning using Optuna Bayesian Optimization.
